{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
      "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
      "1   2     120000    2          2         2   26     -1      2      0      0   \n",
      "2   3      90000    2          2         2   34      0      0      0      0   \n",
      "3   4      50000    2          2         1   37      0      0      0      0   \n",
      "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
      "\n",
      "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
      "0  ...          0          0          0         0       689         0   \n",
      "1  ...       3272       3455       3261         0      1000      1000   \n",
      "2  ...      14331      14948      15549      1518      1500      1000   \n",
      "3  ...      28314      28959      29547      2000      2019      1200   \n",
      "4  ...      20940      19146      19131      2000     36681     10000   \n",
      "\n",
      "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
      "0         0         0         0                           1  \n",
      "1      1000         0      2000                           1  \n",
      "2      1000      1000      5000                           0  \n",
      "3      1100      1069      1000                           0  \n",
      "4      9000       689       679                           0  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "Data preprocessing completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shamb\\AppData\\Local\\Temp\\ipykernel_14992\\642624940.py:14: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'C:\\Users\\shamb\\Downloads\\default+of+credit+card+clients\\default of credit card clients.xls'\n",
    "data = pd.read_excel(file_path, header=1)  # Assuming the first row is the header\n",
    "\n",
    "# Inspect the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Handle missing values (if any)\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# Feature and target separation\n",
    "X = data.drop(columns=['default payment next month'])  # Assuming 'default.payment.next.month' is the target column\n",
    "y = data['default payment next month']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data preprocessing completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shamb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,473</span> (5.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,473\u001b[0m (5.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,473</span> (5.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,473\u001b[0m (5.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 838us/step - accuracy: 0.7127 - loss: 0.5740 - val_accuracy: 0.8127 - val_loss: 0.4602\n",
      "Epoch 2/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - accuracy: 0.8218 - loss: 0.4456 - val_accuracy: 0.8165 - val_loss: 0.4505\n",
      "Epoch 3/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - accuracy: 0.8176 - loss: 0.4456 - val_accuracy: 0.8190 - val_loss: 0.4492\n",
      "Epoch 4/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - accuracy: 0.8238 - loss: 0.4266 - val_accuracy: 0.8167 - val_loss: 0.4482\n",
      "Epoch 5/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - accuracy: 0.8219 - loss: 0.4273 - val_accuracy: 0.8175 - val_loss: 0.4458\n",
      "Epoch 6/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - accuracy: 0.8267 - loss: 0.4172 - val_accuracy: 0.8194 - val_loss: 0.4447\n",
      "Epoch 7/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - accuracy: 0.8239 - loss: 0.4212 - val_accuracy: 0.8175 - val_loss: 0.4417\n",
      "Epoch 8/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - accuracy: 0.8252 - loss: 0.4191 - val_accuracy: 0.8160 - val_loss: 0.4434\n",
      "Epoch 9/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - accuracy: 0.8271 - loss: 0.4210 - val_accuracy: 0.8204 - val_loss: 0.4424\n",
      "Epoch 10/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - accuracy: 0.8213 - loss: 0.4249 - val_accuracy: 0.8152 - val_loss: 0.4425\n",
      "Epoch 11/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - accuracy: 0.8292 - loss: 0.4132 - val_accuracy: 0.8171 - val_loss: 0.4426\n",
      "Epoch 12/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - accuracy: 0.8215 - loss: 0.4236 - val_accuracy: 0.8183 - val_loss: 0.4405\n",
      "Epoch 13/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - accuracy: 0.8222 - loss: 0.4236 - val_accuracy: 0.8142 - val_loss: 0.4428\n",
      "Epoch 14/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 856us/step - accuracy: 0.8217 - loss: 0.4211 - val_accuracy: 0.8173 - val_loss: 0.4455\n",
      "Epoch 15/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8260 - loss: 0.4146 - val_accuracy: 0.8173 - val_loss: 0.4422\n",
      "Epoch 16/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - accuracy: 0.8261 - loss: 0.4163 - val_accuracy: 0.8152 - val_loss: 0.4428\n",
      "Epoch 17/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - accuracy: 0.8221 - loss: 0.4180 - val_accuracy: 0.8169 - val_loss: 0.4413\n",
      "Epoch 18/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 857us/step - accuracy: 0.8302 - loss: 0.4027 - val_accuracy: 0.8156 - val_loss: 0.4403\n",
      "Epoch 19/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - accuracy: 0.8278 - loss: 0.4144 - val_accuracy: 0.8156 - val_loss: 0.4445\n",
      "Epoch 20/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - accuracy: 0.8271 - loss: 0.4123 - val_accuracy: 0.8138 - val_loss: 0.4465\n",
      "Epoch 21/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - accuracy: 0.8301 - loss: 0.4055 - val_accuracy: 0.8131 - val_loss: 0.4437\n",
      "Epoch 22/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - accuracy: 0.8219 - loss: 0.4177 - val_accuracy: 0.8169 - val_loss: 0.4427\n",
      "Epoch 23/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - accuracy: 0.8251 - loss: 0.4120 - val_accuracy: 0.8112 - val_loss: 0.4448\n",
      "Epoch 24/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - accuracy: 0.8279 - loss: 0.4064 - val_accuracy: 0.8129 - val_loss: 0.4415\n",
      "Epoch 25/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - accuracy: 0.8257 - loss: 0.4090 - val_accuracy: 0.8152 - val_loss: 0.4463\n",
      "Epoch 26/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 840us/step - accuracy: 0.8277 - loss: 0.4113 - val_accuracy: 0.8117 - val_loss: 0.4477\n",
      "Epoch 27/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - accuracy: 0.8303 - loss: 0.4047 - val_accuracy: 0.8100 - val_loss: 0.4455\n",
      "Epoch 28/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - accuracy: 0.8258 - loss: 0.4088 - val_accuracy: 0.8146 - val_loss: 0.4453\n",
      "Epoch 29/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - accuracy: 0.8326 - loss: 0.4023 - val_accuracy: 0.8142 - val_loss: 0.4454\n",
      "Epoch 30/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - accuracy: 0.8308 - loss: 0.4025 - val_accuracy: 0.8125 - val_loss: 0.4424\n",
      "Epoch 31/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - accuracy: 0.8256 - loss: 0.4104 - val_accuracy: 0.8129 - val_loss: 0.4448\n",
      "Epoch 32/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - accuracy: 0.8356 - loss: 0.3916 - val_accuracy: 0.8129 - val_loss: 0.4444\n",
      "Epoch 33/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - accuracy: 0.8307 - loss: 0.3976 - val_accuracy: 0.8129 - val_loss: 0.4485\n",
      "Epoch 34/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - accuracy: 0.8280 - loss: 0.4049 - val_accuracy: 0.8123 - val_loss: 0.4458\n",
      "Epoch 35/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - accuracy: 0.8330 - loss: 0.3995 - val_accuracy: 0.8083 - val_loss: 0.4446\n",
      "Epoch 36/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - accuracy: 0.8336 - loss: 0.3968 - val_accuracy: 0.8121 - val_loss: 0.4458\n",
      "Epoch 37/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - accuracy: 0.8308 - loss: 0.3977 - val_accuracy: 0.8108 - val_loss: 0.4500\n",
      "Epoch 38/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - accuracy: 0.8295 - loss: 0.3987 - val_accuracy: 0.8108 - val_loss: 0.4470\n",
      "Epoch 39/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - accuracy: 0.8338 - loss: 0.3950 - val_accuracy: 0.8104 - val_loss: 0.4456\n",
      "Epoch 40/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - accuracy: 0.8293 - loss: 0.3964 - val_accuracy: 0.8096 - val_loss: 0.4527\n",
      "Epoch 41/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - accuracy: 0.8299 - loss: 0.3962 - val_accuracy: 0.8110 - val_loss: 0.4502\n",
      "Epoch 42/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - accuracy: 0.8300 - loss: 0.3932 - val_accuracy: 0.8131 - val_loss: 0.4495\n",
      "Epoch 43/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - accuracy: 0.8356 - loss: 0.3922 - val_accuracy: 0.8081 - val_loss: 0.4485\n",
      "Epoch 44/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - accuracy: 0.8354 - loss: 0.3932 - val_accuracy: 0.8108 - val_loss: 0.4456\n",
      "Epoch 45/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - accuracy: 0.8324 - loss: 0.3935 - val_accuracy: 0.8110 - val_loss: 0.4548\n",
      "Epoch 46/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - accuracy: 0.8353 - loss: 0.3897 - val_accuracy: 0.8069 - val_loss: 0.4519\n",
      "Epoch 47/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - accuracy: 0.8339 - loss: 0.3900 - val_accuracy: 0.8102 - val_loss: 0.4540\n",
      "Epoch 48/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - accuracy: 0.8326 - loss: 0.3913 - val_accuracy: 0.8060 - val_loss: 0.4514\n",
      "Epoch 49/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - accuracy: 0.8388 - loss: 0.3869 - val_accuracy: 0.8069 - val_loss: 0.4545\n",
      "Epoch 50/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - accuracy: 0.8416 - loss: 0.3814 - val_accuracy: 0.8079 - val_loss: 0.4526\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - accuracy: 0.8003 - loss: 0.4614\n",
      "Test Loss: 0.4563373923301697\n",
      "Test Accuracy: 0.8061666488647461\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 00m 04s]\n",
      "val_accuracy: 0.8202083110809326\n",
      "\n",
      "Best val_accuracy So Far: 0.8216666579246521\n",
      "Total elapsed time: 00h 05m 46s\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the input layer is 128 with 3 layers, each having units [112, 16, 16]. The optimal learning rate for the optimizer is 0.001.\n",
      "\n",
      "Epoch 1/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 917us/step - accuracy: 0.7653 - loss: 0.5072 - val_accuracy: 0.8123 - val_loss: 0.4511\n",
      "Epoch 2/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - accuracy: 0.8219 - loss: 0.4322 - val_accuracy: 0.8200 - val_loss: 0.4427\n",
      "Epoch 3/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - accuracy: 0.8238 - loss: 0.4270 - val_accuracy: 0.8177 - val_loss: 0.4453\n",
      "Epoch 4/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - accuracy: 0.8223 - loss: 0.4279 - val_accuracy: 0.8175 - val_loss: 0.4436\n",
      "Epoch 5/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - accuracy: 0.8212 - loss: 0.4258 - val_accuracy: 0.8194 - val_loss: 0.4433\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - accuracy: 0.8148 - loss: 0.4445\n",
      "Test Loss: 0.44060686230659485\n",
      "Test Accuracy: 0.8169999718666077\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'C:\\Users\\shamb\\Downloads\\default+of+credit+card+clients\\default of credit card clients.xls'\n",
    "data = pd.read_excel(file_path, header=1)  # Assuming the first row is the header\n",
    "\n",
    "# Handle missing values (if any)\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# Feature and target separation\n",
    "X = data.drop(columns=['default payment next month'])  # Assuming 'default.payment.next.month' is the target column\n",
    "y = data['default payment next month']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the hypermodel\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('units_input', min_value=16, max_value=128, step=16), activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(Dense(units=hp.Int(f'units_{i}', min_value=16, max_value=128, step=16), activation='relu'))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=50,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='credit_card_fraud')\n",
    "\n",
    "# Perform the hyperparameter tuning\n",
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the input layer is {best_hps.get('units_input')} with {best_hps.get('num_layers')} layers, each having units {[best_hps.get(f'units_{i}') for i in range(best_hps.get('num_layers'))]}. The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directories created at C:\\Users\\shamb\\Desktop\\Ai powered credit card fraud detection\\my_dir\\saved_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the project directory structure\n",
    "project_dir = r'C:\\Users\\shamb\\Desktop\\Ai powered credit card fraud detection\\my_dir\\saved_model'\n",
    "model_dir = os.path.join(project_dir, 'model', 'saved_model')\n",
    "templates_dir = os.path.join(project_dir, 'templates')\n",
    "\n",
    "# Create the directories\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(templates_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Project directories created at {project_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directories created at C:\\Users\\shamb\\Desktop\\Ai powered credit card fraud detection\\my_dir\\saved_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the project directory structure\n",
    "project_dir = r'C:\\Users\\shamb\\Desktop\\Ai powered credit card fraud detection\\my_dir\\saved_model'\n",
    "model_dir = os.path.join(project_dir, 'model', 'saved_model')\n",
    "templates_dir = os.path.join(project_dir, 'templates')\n",
    "\n",
    "# Create the directories\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(templates_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Project directories created at {project_dir}\")\n",
    "\n",
    "# Install dependencies from requirements.txt\n",
    "os.system(f'pip install -r {os.path.join(project_dir, \"requirements.txt\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shamb\\AppData\\Local\\Temp\\ipykernel_15052\\3010178493.py:23: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\credit_card_fraud\\tuner0.json\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the input layer is 112 with 2 layers, each having units [96, 32]. The optimal learning rate for the optimizer is 0.001.\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shamb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7968 - loss: 0.4857 - val_accuracy: 0.8144 - val_loss: 0.4507\n",
      "Epoch 2/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - accuracy: 0.8130 - loss: 0.4465 - val_accuracy: 0.8188 - val_loss: 0.4451\n",
      "Epoch 3/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - accuracy: 0.8233 - loss: 0.4301 - val_accuracy: 0.8213 - val_loss: 0.4456\n",
      "Epoch 4/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - accuracy: 0.8212 - loss: 0.4292 - val_accuracy: 0.8165 - val_loss: 0.4455\n",
      "Epoch 5/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - accuracy: 0.8234 - loss: 0.4222 - val_accuracy: 0.8177 - val_loss: 0.4441\n",
      "Epoch 6/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - accuracy: 0.8201 - loss: 0.4230 - val_accuracy: 0.8138 - val_loss: 0.4476\n",
      "Epoch 7/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - accuracy: 0.8254 - loss: 0.4174 - val_accuracy: 0.8175 - val_loss: 0.4444\n",
      "Epoch 8/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - accuracy: 0.8186 - loss: 0.4173 - val_accuracy: 0.8179 - val_loss: 0.4460\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496us/step - accuracy: 0.8125 - loss: 0.4469\n",
      "Test Loss: 0.44381099939346313\n",
      "Test Accuracy: 0.8161666393280029\n",
      "Model saved to C:\\Users\\shamb\\Desktop\\Ai powered credit card fraud detection\\my_dir\\saved_model\\model\\credit_card_fraud_model.keras\n",
      "Scaler saved to C:\\Users\\shamb\\Desktop\\Ai powered credit card fraud detection\\my_dir\\saved_model\\model\\scaler.joblib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the project directory\n",
    "base_save_dir = r'C:\\Users\\shamb\\Desktop\\Ai powered credit card fraud detection\\my_dir\\saved_model'\n",
    "model_dir = os.path.join(base_save_dir, 'model')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'C:\\Users\\shamb\\Downloads\\default+of+credit+card+clients\\default of credit card clients.xls'\n",
    "data = pd.read_excel(file_path, header=1)  # Assuming the first row is the header\n",
    "\n",
    "# Handle missing values (if any)\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# Feature and target separation\n",
    "X = data.drop(columns=['default payment next month'])  # Adjust column name if necessary\n",
    "y = data['default payment next month']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the hypermodel\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('units_input', min_value=16, max_value=128, step=16), activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(Dense(units=hp.Int(f'units_{i}', min_value=16, max_value=128, step=16), activation='relu'))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=50,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='credit_card_fraud')\n",
    "\n",
    "# Perform the hyperparameter tuning\n",
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the input layer is {best_hps.get('units_input')} with {best_hps.get('num_layers')} layers, each having units {[best_hps.get(f'units_{i}') for i in range(best_hps.get('num_layers'))]}. The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Save the trained model in .keras format\n",
    "model_save_path_keras = os.path.join(model_dir, 'credit_card_fraud_model.keras')\n",
    "model.save(model_save_path_keras)\n",
    "print(f\"Model saved to {model_save_path_keras}\")\n",
    "\n",
    "# Save the scaler parameters\n",
    "scaler_save_path = os.path.join(model_dir, 'scaler.joblib')\n",
    "joblib.dump(scaler, scaler_save_path)\n",
    "print(f\"Scaler saved to {scaler_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shamb\\AppData\\Local\\Temp\\ipykernel_15052\\1645595496.py:20: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data.fillna(method='ffill', inplace=True)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - accuracy: 0.8049 - loss: 0.4442\n",
      "Test Loss: 0.4433768391609192\n",
      "Test Accuracy: 0.8134999871253967\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define the project directory\n",
    "base_save_dir = r'C:\\Users\\shamb\\Desktop\\Ai powered credit card fraud detection\\my_dir'\n",
    "model_path_h5 = os.path.join(base_save_dir, 'credit_card_fraud_model.h5')\n",
    "model_path_keras = os.path.join(base_save_dir, 'credit_card_fraud_model.keras')\n",
    "scaler_path = os.path.join(base_save_dir, 'saved_model', 'model', 'scaler.joblib')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'C:\\Users\\shamb\\Downloads\\default+of+credit+card+clients\\default of credit card clients.xls'\n",
    "data = pd.read_excel(file_path, header=1)\n",
    "\n",
    "# Handle missing values (if any)\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# Feature and target separation\n",
    "X = data.drop(columns=['default payment next month'])\n",
    "y = data['default payment next month']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Load the model (choose the appropriate path)\n",
    "if os.path.exists(model_path_h5):\n",
    "    model = load_model(model_path_h5)\n",
    "elif os.path.exists(model_path_keras):\n",
    "    model = tf.keras.models.load_model(model_path_keras)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Model file not found\")\n",
    "\n",
    "# Load the saved scaler\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from C:\\Users\\shamb\\Desktop\\Ai powered credit card fraud detection\\my_dir\\saved_model\\model\\credit_card_fraud_model.keras\n",
      "Scaler loaded from C:\\Users\\shamb\\Desktop\\Ai powered credit card fraud detection\\my_dir\\saved_model\\model\\scaler.joblib\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step\n",
      "Transaction 1 is Default\n",
      "Transaction 2 is Default\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define paths for the saved model and scaler\n",
    "base_save_dir = r'C:\\Users\\shamb\\Desktop\\Ai powered credit card fraud detection\\my_dir'\n",
    "model_path = os.path.join(base_save_dir, 'saved_model', 'model', 'credit_card_fraud_model.keras')\n",
    "scaler_path = os.path.join(base_save_dir, 'saved_model', 'model', 'scaler.joblib')\n",
    "\n",
    "# Check if the model and scaler files exist\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "if not os.path.exists(scaler_path):\n",
    "    raise FileNotFoundError(f\"Scaler file not found at {scaler_path}\")\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(model_path)\n",
    "print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "# Load the scaler\n",
    "scaler = joblib.load(scaler_path)\n",
    "print(f\"Scaler loaded from {scaler_path}\")\n",
    "\n",
    "# Assume we have new data for which we need predictions\n",
    "# Example of new input data (replace this with actual new data)\n",
    "new_data = pd.DataFrame({\n",
    "    'ID': [1, 2],  # Add ID column\n",
    "    'LIMIT_BAL': [50000, 200000],  # Add LIMIT_BAL column\n",
    "    'SEX': [1, 2],                 # Example values for gender\n",
    "    'EDUCATION': [2, 3],           # Example values for education\n",
    "    'MARRIAGE': [1, 2],            # Example values for marriage status\n",
    "    'AGE': [25, 40],               # Example values for age\n",
    "    'PAY_0': [1, 2],\n",
    "    'PAY_2': [1, 2],\n",
    "    'PAY_3': [1, 2],\n",
    "    'PAY_4': [1, 2],\n",
    "    'PAY_5': [1, 2],\n",
    "    'PAY_6': [1, 2],\n",
    "    'BILL_AMT1': [1000, 2000],\n",
    "    'BILL_AMT2': [500, 1000],\n",
    "    'BILL_AMT3': [2000, 3000],\n",
    "    'BILL_AMT4': [1500, 2500],\n",
    "    'BILL_AMT5': [3000, 4000],\n",
    "    'BILL_AMT6': [2500, 3500],\n",
    "    'PAY_AMT1': [500, 1000],\n",
    "    'PAY_AMT2': [1000, 2000],\n",
    "    'PAY_AMT3': [1500, 2500],\n",
    "    'PAY_AMT4': [2000, 3000],\n",
    "    'PAY_AMT5': [2500, 3500],\n",
    "    'PAY_AMT6': [3000, 4000]\n",
    "})\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    new_data[col] = le.fit_transform(new_data[col])\n",
    "\n",
    "# Normalize the new data using the loaded scaler\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(new_data_scaled)\n",
    "\n",
    "# Interpret predictions (0: Not Default, 1: Default)\n",
    "predicted_classes = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Print the results\n",
    "for i, prediction in enumerate(predicted_classes):\n",
    "    print(f\"Transaction {i+1} is {'Default' if prediction == 1 else 'Not Default'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
